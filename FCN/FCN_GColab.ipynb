{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCN_GColab.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j3jbbpXjF9Ei","colab_type":"text"},"source":["#CellStrat AI Lab\n","\n","##Semantic Image Segmentation - FCN model for Kitty Roads dataset"]},{"cell_type":"code","metadata":{"id":"JTC6d1F8qDnU","colab_type":"code","colab":{}},"source":["# %load main.py\n","#--------------------------\n","# USER-SPECIFIED DATA\n","#CellStrat\n","#Reference blog : https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n","# Dataset download : Download the Kitti Road dataset from http://www.cvlibs.net/download.php?file=data_road.zip\n","# In this model we will run 40 epochs on the entire dataset. In Colab it takes 10-15 minutes to run\n","# On a personal computer, just with 5 training images and 40 training epochs, this program took 6-8 hours.\n","# Better to run it on the Cloud\n","#--------------------------\n","\n","# Tune these parameters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6O7OriHqXwg","colab_type":"code","outputId":"d02c4dc4-d529-46b7-c66a-54021bcc2d7a","executionInfo":{"status":"ok","timestamp":1566125343395,"user_tz":-330,"elapsed":7982,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","!pwd\n","\n","import os\n","os.chdir('/content/gdrive/My Drive/Colab Notebooks/FCN_ImageSegmentation/FCN')\n","!pwd"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Colab Notebooks/FCN_ImageSegmentation/FCN\n","/content/gdrive/My Drive/Colab Notebooks/FCN_ImageSegmentation/FCN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MvQ3BTreE_vW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"9097e4fd-d5f2-4417-df18-1d2aaca821ac","executionInfo":{"status":"ok","timestamp":1566125350537,"user_tz":-330,"elapsed":4338,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}}},"source":["!ls"],"execution_count":25,"outputs":[{"output_type":"stream","text":[" data\t\t 'helper.py (2).bak'\t   main.ipynb\t      README.md\n"," examples\t  helper.py.bak3\t   main.py\t      README-Orig.md\n"," _gitignore.txt   LICENSE\t\t   project_tests.py   runs\n"," helper.py\t  long_shelhamer_fcn.pdf   __pycache__\t      set_git.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M1oKjV1krlP9","colab_type":"code","outputId":"7648012a-643d-42a9-fe90-eb5618571b17","executionInfo":{"status":"ok","timestamp":1566125358046,"user_tz":-330,"elapsed":6206,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["pip install Pillow"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o-rECORC6KhH","colab_type":"code","outputId":"7397fe03-941d-4a2c-a5b1-df60fb8f8201","executionInfo":{"status":"ok","timestamp":1566125366229,"user_tz":-330,"elapsed":9952,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}},"colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["!pip install scikit-image\n","#pip install skimage\n","#CellStrat - use older version of scipy which has APIs like imresize, imsave, toimage, imsave etc.\n","!pip install scipy==1.2.1"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.15.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.2.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (4.3.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.0.3)\n","Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.3)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.0.3)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17.0->scikit-image) (1.16.4)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image) (0.46)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.5.3)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (41.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n","Requirement already satisfied: scipy==1.2.1 in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.1) (1.16.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tpJswcS4qDnZ","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import helper\n","\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"otxUY_7uqDnd","colab_type":"code","outputId":"c3f10042-a98f-4c3c-fd2a-89ee9a2a16ab","executionInfo":{"status":"ok","timestamp":1566125985513,"user_tz":-330,"elapsed":621993,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["tf.reset_default_graph()\n","\n","#CellStrat\n","num_classes = 2\n","NUMBER_OF_CLASSES = 2\n","image_shape = (160, 576)\n","IMAGE_SHAPE = (160, 576)\n","EPOCHS = 20\n","BATCH_SIZE = 10\n","DROPOUT = 0.75\n","\n","# Specify these directory paths\n","\n","data_dir = './data'\n","runs_dir = './runs'\n","training_dir ='./data/data_road/training'\n","vgg_path = './data/vgg'\n","\n","#--------------------------\n","# PLACEHOLDER TENSORS\n","#--------------------------\n","\n","correct_label = tf.placeholder(tf.float32, [None, IMAGE_SHAPE[0], IMAGE_SHAPE[1], NUMBER_OF_CLASSES])\n","learning_rate = tf.placeholder(tf.float32)\n","keep_prob = tf.placeholder(tf.float32)\n","\n","#--------------------------\n","# FUNCTIONS\n","#--------------------------\n","\n","def load_vgg(sess, vgg_path):\n","  \n","  # load the model and weights\n","  model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n","\n","  # Get Tensors to be returned from graph\n","  graph = tf.get_default_graph()\n","  image_input = graph.get_tensor_by_name('image_input:0')\n","  keep_prob = graph.get_tensor_by_name('keep_prob:0')\n","  layer3 = graph.get_tensor_by_name('layer3_out:0')\n","  layer4 = graph.get_tensor_by_name('layer4_out:0')\n","  layer7 = graph.get_tensor_by_name('layer7_out:0')\n","\n","  return image_input, keep_prob, layer3, layer4, layer7\n","\n","def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n","   \n","    # Use a shorter variable name for simplicity\n","    layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out\n","\n","    # Apply 1x1 convolution in place of fully connected layer\n","    fcn8 = tf.layers.conv2d(layer7, filters=num_classes, kernel_size=1, name=\"fcn8\")\n","\n","    # Upsample fcn8 with size depth=(4096?) to match size of layer 4 so that we can add skip connection with 4th layer\n","    fcn9 = tf.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1],\n","    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn9\")\n","\n","    # Add a skip connection between current final layer fcn8 and 4th layer\n","    fcn9_skip_connected = tf.add(fcn9, layer4, name=\"fcn9_plus_vgg_layer4\")\n","\n","    # Upsample again\n","    fcn10 = tf.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1],\n","    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn10_conv2d\")\n","\n","    # Add skip connection\n","    fcn10_skip_connected = tf.add(fcn10, layer3, name=\"fcn10_plus_vgg_layer3\")\n","\n","    # Upsample again\n","    fcn11 = tf.layers.conv2d_transpose(fcn10_skip_connected, filters=num_classes,\n","    kernel_size=16, strides=(8, 8), padding='SAME', name=\"fcn11\")\n","\n","    return fcn11\n","\n","def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n","  \n","  # Reshape 4D tensors to 2D, each row represents a pixel, each column a class\n","  logits = tf.reshape(nn_last_layer, (-1, num_classes), name=\"fcn_logits\")\n","  correct_label_reshaped = tf.reshape(correct_label, (-1, num_classes))\n","\n","  # Calculate distance from actual labels using cross entropy\n","  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=correct_label_reshaped[:])\n","  # Take mean for total loss\n","  loss_op = tf.reduce_mean(cross_entropy, name=\"fcn_loss\")\n","\n","  # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n","  train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op, name=\"fcn_train_op\")\n","\n","  return logits, train_op, loss_op\n","\n","def train_nn(sess, epochs, batch_size, get_batches_fn, train_op,\n","             cross_entropy_loss, input_image,\n","             correct_label, keep_prob, learning_rate):\n","\n","  keep_prob_value = 0.5\n","  learning_rate_value = 0.001\n","  for epoch in range(epochs):\n","      # Create function to get batches\n","      total_loss = 0\n","      for X_batch, gt_batch in get_batches_fn(batch_size):\n","\n","          loss, _ = sess.run([cross_entropy_loss, train_op],\n","          feed_dict={input_image: X_batch, correct_label: gt_batch,\n","          keep_prob: keep_prob_value, learning_rate:learning_rate_value})\n","\n","          total_loss += loss;\n","\n","      print(\"EPOCH {} ...\".format(epoch + 1))\n","      print(\"Loss = {:.3f}\".format(total_loss))\n","      print()\n","\n","def run():\n","  \n","  # Download pretrained vgg model\n","  helper.maybe_download_pretrained_vgg(data_dir)\n","\n","  # A function to get batches\n","  get_batches_fn = helper.gen_batch_function(training_dir, image_shape)\n","  \n","  with tf.Session() as session:\n","        \n","    # Returns the three layers, keep probability and input layer from the vgg architecture\n","    image_input, keep_prob, layer3, layer4, layer7 = load_vgg(session, vgg_path)\n","\n","    # The resulting network architecture from adding a decoder on top of the given vgg model\n","    model_output = layers(layer3, layer4, layer7, num_classes)\n","\n","    # Returns the output logits, training operation and cost operation to be used\n","    # - logits: each row represents a pixel, each column a class\n","    # - train_op: function used to get the right parameters to the model to correctly label the pixels\n","    # - cross_entropy_loss: function outputting the cost which we are minimizing, lower cost should yield higher accuracy\n","    logits, train_op, cross_entropy_loss = optimize(model_output, correct_label, learning_rate, num_classes)\n","    \n","    # Initialize all variables\n","    session.run(tf.global_variables_initializer())\n","    session.run(tf.local_variables_initializer())\n","\n","    print(\"Model build successful, starting training\")\n","\n","    # Train the neural network\n","    train_nn(session, EPOCHS, BATCH_SIZE, get_batches_fn, \n","             train_op, cross_entropy_loss, image_input,\n","             correct_label, keep_prob, learning_rate)\n","\n","    # Run the model with the test images and save each painted output image (roads painted green)\n","    helper.save_inference_samples(runs_dir, data_dir, session, image_shape, logits, keep_prob, image_input)\n","    \n","    print(\"All done!\")\n","\n","#--------------------------\n","# MAIN\n","#--------------------------\n","if __name__ == '__main__':\n","    run()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Model build successful, starting training\n","EPOCH 1 ...\n","Loss = 161.430\n","\n","EPOCH 2 ...\n","Loss = 6.955\n","\n","EPOCH 3 ...\n","Loss = 5.361\n","\n","EPOCH 4 ...\n","Loss = 4.652\n","\n","EPOCH 5 ...\n","Loss = 4.369\n","\n","EPOCH 6 ...\n","Loss = 4.129\n","\n","EPOCH 7 ...\n","Loss = 4.059\n","\n","EPOCH 8 ...\n","Loss = 3.945\n","\n","EPOCH 9 ...\n","Loss = 3.790\n","\n","EPOCH 10 ...\n","Loss = 3.710\n","\n","EPOCH 11 ...\n","Loss = 3.425\n","\n","EPOCH 12 ...\n","Loss = 3.457\n","\n","EPOCH 13 ...\n","Loss = 3.366\n","\n","EPOCH 14 ...\n","Loss = 3.123\n","\n","EPOCH 15 ...\n","Loss = 2.904\n","\n","EPOCH 16 ...\n","Loss = 2.818\n","\n","EPOCH 17 ...\n","Loss = 2.728\n","\n","EPOCH 18 ...\n","Loss = 2.618\n","\n","EPOCH 19 ...\n","Loss = 2.632\n","\n","EPOCH 20 ...\n","Loss = 2.689\n","\n","Training Finished. Saving test images to: ./runs/1566125985.2485893\n","All done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"adBRYG1AqDnf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"outputId":"23a3ba23-1c84-41aa-9ded-2bec309c0d66","executionInfo":{"status":"error","timestamp":1566126336433,"user_tz":-330,"elapsed":1152,"user":{"displayName":"jani basha","photoUrl":"https://lh5.googleusercontent.com/-X9SSPHQ_JFo/AAAAAAAAAAI/AAAAAAAADqE/hXKQG_LqnPk/s64/photo.jpg","userId":"01940010228818985276"}}},"source":["layers.summary()"],"execution_count":31,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-6e6143c713c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'summary'"]}]},{"cell_type":"code","metadata":{"id":"8rRUNSAHagCz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}